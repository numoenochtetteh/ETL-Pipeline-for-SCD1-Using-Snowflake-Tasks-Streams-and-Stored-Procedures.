v=# 🚀 ETL Pipeline for SCD1 Using Snowflake Tasks, Streams & Stored Procedures

---

## 📖 Project Overview

This project demonstrates how to build a **real-time ETL pipeline** that applies **Slowly Changing Dimension Type 1 (SCD1)** logic using:

- 🧊 **Snowflake** — Cloud Data Warehouse  
- ☁️ **AWS S3** — Raw Data Storage  
- 🔄 **Snowpipe** — Automatic Data Ingestion  
- 📥 **Streams & Tasks** — Change Detection & Scheduling  
- 📜 **Stored Procedures** — SCD1 Logic Implementation  

The goal is to ingest data from AWS S3 into Snowflake and transform it to maintain the latest version of records in a dimension table.

---

## ✅ Project Setup (All Completed)

### 🔧 Configuration Steps

- ✅ **Snowflake Account Setup**  
- ✅ **AWS Account Setup**  
- ✅ **Anaconda Cloud Setup**  
- ✅ **AWS IAM User Configuration**  
- ✅ **AWS S3 Bucket Setup**  
- ✅ **AWS S3 ↔ Snowflake Integration**  
- ✅ **Snowpipe Setup**  
- ✅ **Stream Setup**  
- ✅ **Task Setup**  
- ✅ **Testing of SCD1 Logic**

---

## 🧭 How the Project Works – Step-by-Step

### 1️⃣ Step 1: Setup Snowflake and AWS  
Create accounts and configure IAM roles/policies to allow secure communication between AWS and Snowflake.

---

### 2️⃣ Step 2: Upload Raw Data to S3  
Simulate incoming data by uploading `.csv` or `.json` files to your S3 bucket.

---

### 3️⃣ Step 3: Create Snowflake Stage & File Format  
Create an external stage in Snowflake and define the file format to connect with your S3 bucket.

---

### 4️⃣ Step 4: Build Snowpipe for Auto-Loading  
Use Snowpipe to automatically ingest data into a Snowflake **landing table**.

---

### 5️⃣ Step 5: Create a Stream  
Create a stream on the landing table to track new or changed rows.

---

### 6️⃣ Step 6: Write a Stored Procedure  
Write a stored procedure that applies **SCD1 logic**:  
- Compares records from stream with dimension table  
- Updates if key matches  
- Inserts if new record

---

### 7️⃣ Step 7: Automate with Snowflake Task  
Schedule a task to run the stored procedure regularly (e.g., every 5 minutes) to ensure continuous updates.

---

### 8️⃣ Step 8: Test the Full Pipeline  
Upload new files to S3 and validate:
- ✅ Snowpipe ingests to landing table  
- ✅ Stream detects new rows  
- ✅ Task runs stored procedure  
- ✅ Dimension table reflects SCD1 updates

---
