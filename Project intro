

# üöÄ ETL Pipeline for SCD1 Using Snowflake Tasks, Streams & Stored Procedures

---

## üìñ Project Overview

This project demonstrates how to build a **real-time ETL pipeline** that applies **Slowly Changing Dimension Type 1 (SCD1)** logic using:

* üßä **Snowflake** ‚Äî as the cloud data warehouse
* ‚òÅÔ∏è **AWS S3** ‚Äî as the raw data storage
* üîÑ **Snowpipe** ‚Äî for automatic data ingestion
* üì• **Streams & Tasks** ‚Äî to detect and automate changes
* üìú **Stored Procedures** ‚Äî to apply SCD1 transformation logic

The goal is to ingest data from AWS S3 into Snowflake and transform it to maintain the latest version of records in a dimension table.

---

## ‚úÖ Project Setup (All Completed)

### üßä Snowflake & AWS Configurations

* ‚úÖ **[Snowflake Account Setup](#)**
* ‚úÖ **[AWS Account Setup](#)**
* ‚úÖ **[Anaconda Cloud Setup](#)**
* ‚úÖ **[AWS IAM User Configuration](#)**
* ‚úÖ **[AWS S3 Bucket Setup](#)**

### üîó Integrations & Pipeline Components

* ‚úÖ **[S3 ‚Üî Snowflake Integration](#)**
* ‚úÖ **[Snowpipe Setup](#)**
* ‚úÖ **[Stream Setup](#)**
* ‚úÖ **[Task Setup](#)**
* ‚úÖ **[Testing of SCD1 Logic](#)**

> *Replace `#` with actual links or headings once the README is complete.*

---

## üß≠ How the Project Works ‚Äì Step-by-Step

### 1Ô∏è‚É£ Step 1: Setup Snowflake and AWS

Create Snowflake and AWS accounts. Set up IAM roles and permissions to allow S3 ‚Üî Snowflake communication.

---

### 2Ô∏è‚É£ Step 2: Upload Raw Data to S3

Upload raw data (CSV or JSON) to an S3 bucket, simulating a new data stream.

---

### 3Ô∏è‚É£ Step 3: Create Snowflake Stage & File Format

Set up an external stage and file format to connect Snowflake with your S3 bucket.

---

### 4Ô∏è‚É£ Step 4: Build Snowpipe for Auto-Loading

Use Snowpipe to automatically ingest files from S3 into a **landing table** in Snowflake.

---

### 5Ô∏è‚É£ Step 5: Create a Stream

Set up a Stream to track new or changed records coming into the landing table.

---

### 6Ô∏è‚É£ Step 6: Write a Stored Procedure (SCD1 Logic)

Create a stored procedure that:

* Compares incoming rows with the target table
* Updates existing rows if keys match
* Inserts new records if no match is found

---

### 7Ô∏è‚É£ Step 7: Schedule with Snowflake Task

Set up a Task to call the stored procedure at intervals (e.g., every 5 minutes), ensuring continuous updates.

---

### 8Ô∏è‚É£ Step 8: Testing the Full Pipeline

Upload new data to S3 and verify the entire pipeline:

* Snowpipe loads the data
* Stream captures the changes
* Task triggers the SCD1 logic
* Dimension table reflects the latest values


